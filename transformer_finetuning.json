{
  "components": {
    "comp-finetuning": {
      "executorLabel": "exec-finetuning"
    }
  },
  "defaultPipelineRoot": "gs://sbx-196865-genaift-ds-pkgs/pipeline_root/",
  "deploymentSpec": {
    "executors": {
      "exec-finetuning": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "finetuning"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'bitsandbytes==0.42.0' 'peft==0.8.2' 'trl==0.7.10' 'accelerate==0.27.1' 'datasets==2.17.0' 'transformers==4.38.0' 'huggingface_hub' 'google-cloud-storage' 'google-cloud-aiplatform' 'google-cloud-pipeline-components' 'gcsfs' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef finetuning():\n    import torch\n    import json\n    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n    from datasets import load_dataset\n    from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n    import bitsandbytes as bnb\n    from peft import LoraConfig, get_peft_model\n    import transformers\n    from trl import SFTTrainer\n    from google.cloud import storage\n    from huggingface_hub import login\n    import os\n\n    def load_json_from_gcs(bucket_name, folder_path, file_name):\n        # Initialize a client\n        client = storage.Client()\n\n        # Get the bucket\n        bucket = client.bucket(bucket_name)\n\n        # Get the blob (file) from the bucket\n        blob = bucket.blob(f\"{folder_path}/{file_name}\")\n\n        # Download the content of the blob as a string\n        json_content = blob.download_as_string()\n\n        # Decode the content to UTF-8 and load it as JSON\n        json_data = json.loads(json_content.decode('utf-8'))\n\n        return json_data\n\n    def write_json_to_local(json_data, file_name):\n        with open(file_name, 'w') as f:\n            json.dump(json_data, f, indent=4)\n    bucket_name = '19865_finetuned_models'\n    folder_path = 'training_data'\n    file_name = 'clean_data.json'\n    # gs://19865_finetuned_models/training_data/clean_data.json\n    # Load JSON from GCS bucket\n    json_data = load_json_from_gcs(bucket_name, folder_path, file_name)\n\n    # Write JSON data to local file with the same name\n    write_json_to_local(json_data, file_name)\n\n    login(token='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')\n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n\n    model_id = \"google/gemma-7b-it\"\n    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}) ## changed this line\n    # model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map='auto')\n    tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n    # dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\n    dataset = load_dataset('json', data_files=file_name)\n\n    def upload_folder_to_gcs(local_folder_path, gcs_bucket, gcs_folder_path=\"\"):\n        # Create a GCS client and bucket object\n        storage_client = storage.Client()\n        bucket = storage_client.bucket(gcs_bucket)\n\n        # Loop through the files in the local folder\n        for root, dirs, files in os.walk(local_folder_path):\n            for file in files:\n                # Construct the local and GCS paths for the file\n                local_path = os.path.join(root, file)\n                gcs_path = os.path.join(gcs_folder_path, local_path[len(local_folder_path)+1:])\n\n                # Upload the file to GCS\n                blob = bucket.blob(gcs_path)\n                blob.upload_from_filename(local_path)\n\n        print(f\"Folder {local_folder_path} uploaded to GCS bucket '{gcs_bucket}' with path '{gcs_folder_path}'\")\n\n    def generate_prompt(data_point):\n        \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n\n        :param data_point: dict: Data point\n        :return: dict: tokenzed prompt\n        \"\"\"\n        prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n                   'appropriately completes the request.\\n\\n'\n        # Samples with additional context into.\n        if data_point['input_text']:\n            text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"input_text\"]} here are the inputs {data_point[\"Data\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output_text\"]} <end_of_turn>\"\"\"\n        # Without\n        else:\n            text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"input_text\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"Data\"]} <end_of_turn>\"\"\"\n        return text\n    def find_all_linear_names(model):\n        cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n        lora_module_names = set()\n        for name, module in model.named_modules():\n            if isinstance(module, cls):\n                names = name.split('.')\n                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n            if 'lm_head' in lora_module_names: # needed for 16-bit\n                lora_module_names.remove('lm_head')\n        return list(lora_module_names)\n    text_column = [generate_prompt(data_point) for data_point in dataset['train']]\n    dataset = dataset['train'].add_column(\"prompt\", text_column)\n    dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n    dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n    dataset = dataset.train_test_split(test_size=0.2)\n    train_data = dataset[\"train\"]\n    test_data = dataset[\"test\"]\n    model.gradient_checkpointing_enable()\n    model = prepare_model_for_kbit_training(model)\n\n    modules = find_all_linear_names(model)\n    lora_config = LoraConfig(\n        r=64,\n        lora_alpha=32,\n        target_modules=modules,\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\")\n    model = get_peft_model(model, lora_config)\n    trainable, total = model.get_nb_trainable_parameters()\n    print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n    tokenizer.pad_token = tokenizer.eos_token\n    torch.cuda.empty_cache()\n    trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n        eval_dataset=test_data,\n        dataset_text_field=\"prompt\",\n        peft_config=lora_config,\n        args=transformers.TrainingArguments(\n            per_device_train_batch_size=1,\n            gradient_accumulation_steps=4,\n            warmup_steps=0.03,\n            num_train_epochs=3,\n            max_steps=100,\n            learning_rate=2e-4,\n            logging_steps=1,\n            output_dir=\"outputs\",\n            optim=\"paged_adamw_8bit\",\n            save_strategy=\"epoch\",\n        ),\n        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    )\n    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n    print(\"*****training started*****\")\n    trainer.train()\n    new_model = \"gemma-Instruct-Finetune-full-acn\"\n\n    trainer.model.save_pretrained(new_model)\n    # trainer.model.push_to_hub(new_model, use_temp_dir=False)\n    # tokenizer.push_to_hub(new_model, use_temp_dir=False)\n    # print('*****trained_model_pushed_to_hub*****')\n\n    base_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},)\n\n    merged_model= PeftModel.from_pretrained(base_model, new_model)\n    merged_model= merged_model.merge_and_unload()\n\n    merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n    tokenizer.save_pretrained(\"merged_model\")\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    merged_model.push_to_hub(new_model, use_temp_dir=False)\n    tokenizer.push_to_hub(new_model, use_temp_dir=False)\n\n\n    # local_path = f\"transformers/{new_model}\"\n    # trainer.model.save_pretrained(local_path)\n\n    # upload_folder_to_gcs('transformers','19865_finetuned_models','transformers_finetuned_models')\n    # print(\"*****training_completed*****\")\n    # # trainer.model.save_pretrained(new_model)\n\n"
          ],
          "image": "gcr.io/deeplearning-platform-release/base-cu113.py310",
          "resources": {
            "accelerator": {
              "count": "1",
              "type": "NVIDIA_TESLA_A100"
            },
            "cpuLimit": 12.0,
            "memoryLimit": 85.0
          }
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "finetuning gemma 7b",
    "name": "gemma-finetuning"
  },
  "root": {
    "dag": {
      "tasks": {
        "finetuning": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-finetuning"
          },
          "taskInfo": {
            "name": "finetuning"
          }
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.5.0"
}