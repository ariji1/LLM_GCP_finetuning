{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecc91a00-29d2-46cd-a067-bc56d82db6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import List, Tuple\n",
    "\n",
    "import os\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component,pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a79045-87d0-4306-a96e-5ecf6a0300f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI=\"gs://sbx-196865-genaift-ds-pkgs\"\n",
    "project_id = \"sbx-196865-genaift-ds-ccd784e6\"\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/\".format(BUCKET_URI)\n",
    "EXPERIMENT_NAME = \"test-1\"\n",
    "location = 'us-central1'\n",
    "service_account=\"sa-196865-big-data@sbx-196865-genaift-ds-ccd784e6.iam.gserviceaccount.com\"\n",
    "model_display_name = 'tuned_bison001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7f1c9e2-a6ef-4c2d-8ccb-574de02ef49b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting property name enclosed in double quotes: line 1 column 134 (char 133). Skipping the current entry.\n",
      "Error decoding JSON: Extra data: line 1 column 8 (char 7). Skipping the current entry.\n",
      "Error decoding JSON: Extra data: line 1 column 9 (char 8). Skipping the current entry.\n",
      "Error decoding JSON: Extra data: line 1 column 9 (char 8). Skipping the current entry.\n"
     ]
    }
   ],
   "source": [
    "def load_files_into_string(file_list):\n",
    "    \"\"\"\n",
    "    Load text files from a list into a single string.\n",
    "\n",
    "    Args:\n",
    "        file_list (list): List of file paths.\n",
    "\n",
    "    Returns:\n",
    "        str: Contents of all files concatenated into a single string.\n",
    "    \"\"\"\n",
    "    concatenated_string = \"\"\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_contents = file.read()\n",
    "                concatenated_string += file_contents\n",
    "        except IOError:\n",
    "            print(\"Error: Unable to read file -\", file_path)\n",
    "            continue\n",
    "    return concatenated_string\n",
    "# Example usage:\n",
    "file_list = [\"train_questions.txt\", \"train_questions_1.txt\", \"train_questions_2.txt\",\"train_questions_3.txt\"]\n",
    "all_text = load_files_into_string(file_list)\n",
    "\n",
    "import json\n",
    "\n",
    "def clean_and_load_to_list(string_data):\n",
    "    \"\"\"\n",
    "    Clean the string containing JSON objects and load them into a list.\n",
    "\n",
    "    Args:\n",
    "        string_data (str): String containing JSON objects.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing cleaned data.\n",
    "    \"\"\"\n",
    "    question = []\n",
    "    instruction = []\n",
    "    answer = []\n",
    "    \n",
    "    \n",
    "    cleaned_list = []\n",
    "    # Split the string by newline character to separate individual JSON objects\n",
    "    json_objects = string_data.strip().split('\\n')\n",
    "    for json_str in json_objects:\n",
    "        try:\n",
    "            # Parse each JSON object\n",
    "            json_data = json.loads(json_str)\n",
    "            # Extract required fields\n",
    "            question = json_data.get('question', 'N/A')\n",
    "            table = json_data.get('table', 'N/A')\n",
    "            answer = json_data.get('answer', 'N/A')\n",
    "            # clause = json_data.get('clause', 'N/A')\n",
    "            \n",
    "            # Append cleaned data to the list\n",
    "            # cleaned_list.append({'text':f'### Human: {question} Here is the Data : {table} ### Assistant: {answer}'})\n",
    "            \n",
    "            cleaned_list.append({\"text\":f'Question: {question} Data : {table} output_text: {answer}'})\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}. Skipping the current entry.\")\n",
    "    return cleaned_list\n",
    "\n",
    "# Example usage:\n",
    "cleaned_data = clean_and_load_to_list(all_text)\n",
    "# print(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd0d6871-2807-4aa0-ac68-8782cf9715ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Question: How are long-lived assets assessed for recoverability according to the provided financial data? Data : | Fiscal Year | Research and Development Costs | Advertising Costs | Provision for Doubtful Accounts |\\n|---|---|---|---|\\n| 2021 | $1,118,320 | $171,883 | $6,199 |\\n| 2020 | $870,611 | $57,658 | $147 |\\n| 2019 | $799,734 | $85,521 | $974 | output_text: Long-lived assets or groups of assets are assessed based on a comparison of the carrying amount to the estimated future net cash flows. If estimated future undiscounted net cash flows are less than the carrying amount, the asset is considered impaired and a loss is recorded. Intangible assets with finite lives are generally amortized using the straight-line method over their estimated economic useful lives.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6735d8e5-3d07-4782-8c3e-37d5028d449e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36856e7a-22ac-4a68-b8e5-b7846dc75464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6aa14dff-a26f-4a5c-96fe-0a0ad42e17c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d046850f-5243-4f51-bb4f-70c3dd618503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset  = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aab3610-57c1-47d3-a46c-239475a52bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf6c1412fe64b2c99c41a95a3817cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e299a757f56d426297293c1d46f461f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6087669e131c4fdf8f9ef21c7afafe8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ce5d823f564f828b934eb335380d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2c3e9a760a4ee9a6cb5716877126f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ariji1/acn_train_Test/commit/0f50ef1b613a650c45285a73ab5932b95e260757', commit_message='Upload dataset', commit_description='', oid='0f50ef1b613a650c45285a73ab5932b95e260757', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"ariji1/acn_train_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d690c6be-9102-4de2-9c98-63ac02e4c943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('clean_data.json', 'w') as json_file:\n",
    "    json.dump(cleaned_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35868cf2-e075-475a-befc-61b02c78a9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from google.cloud import storage\n",
    "\n",
    "def load_json_from_gcs(bucket_name, folder_path, file_name):\n",
    "    # Initialize a client\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Get the blob (file) from the bucket\n",
    "    blob = bucket.blob(f\"{folder_path}/{file_name}\")\n",
    "\n",
    "    # Download the content of the blob as a string\n",
    "    json_content = blob.download_as_string()\n",
    "\n",
    "    # Decode the content to UTF-8 and load it as JSON\n",
    "    json_data = json.loads(json_content.decode('utf-8'))\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def write_json_to_local(json_data, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(json_data, f, indent=4)\n",
    "\n",
    "# Replace 'your-bucket-name' with your GCS bucket name\n",
    "# Replace 'your-folder-path' with the path to the folder inside the bucket\n",
    "# Replace 'your-file-name.json' with the name of your JSON file in the folder\n",
    "bucket_name = '19865_finetuned_models'\n",
    "folder_path = 'training_data'\n",
    "file_name = 'clean_data.json'\n",
    "# gs://19865_finetuned_models/training_data/clean_data.json\n",
    "# Load JSON from GCS bucket\n",
    "json_data = load_json_from_gcs(bucket_name, folder_path, file_name)\n",
    "\n",
    "# Write JSON data to local file with the same name\n",
    "write_json_to_local(json_data, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9df8ce-81a7-4c68-b833-9907e2c3e7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a1f7157-194d-402b-9116-149cc5f8286e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e17ad48eb841768288f8f48b903f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='clean_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945629bc-d77b-417b-8d6f-0546ae99b37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e844aa62-44a2-4c56-a4cb-f05790109da2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bb1efae41e440a9b428358f104c7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "model_id = \"google/gemma-7b-it\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "dataset = load_dataset('json', data_files='clean_data.json')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "def generate_prompt(data_point):\n",
    "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "    :param data_point: dict: Data point\n",
    "    :return: dict: tokenzed prompt\n",
    "    \"\"\"\n",
    "    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n",
    "               'appropriately completes the request.\\n\\n'\n",
    "    # Samples with additional context into.\n",
    "    if data_point['input_text']:\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"input_text\"]} here are the inputs {data_point[\"Data\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output_text\"]} <end_of_turn>\"\"\"\n",
    "    # Without\n",
    "    else:\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"input_text\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"Data\"]} <end_of_turn>\"\"\"\n",
    "    return text\n",
    "\n",
    "text_column = [generate_prompt(data_point) for data_point in dataset['train']]\n",
    "dataset = dataset['train'].add_column(\"prompt\", text_column)\n",
    "dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82bbe2c2-9108-412c-bcc9-62e5246e206f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3228784-e6c5-4bd6-a698-00a76ff129cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "469011ec-e090-44ac-9afe-579fd55fc5e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text', 'Data', 'output_text', 'prompt'],\n",
       "    num_rows: 154\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69a4d8ab-65e5-42d3-ab15-e639161af04c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599bbe63-c94b-4760-a82d-4834c5f667dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import List, Tuple\n",
    "\n",
    "import os\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component,pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c08165-f544-4dd9-a774-6b300a7599b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('ariji1/acn-finetuning')\n",
    "dataset = data['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191096a6-da0d-49db-9c9a-00cbec61f083",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 123\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 31\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad6d17-a8c7-40d5-bdc6-9ba891a21d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3edafbba-b4b7-45c2-b8d0-01335ef9af9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "__file__ = 'kfp_finetuning.ipynb'\n",
    "__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045a4b26-2ea8-419b-9a78-69df1db46ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_960137/1528039428.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(packages_to_install=['bitsandbytes==0.42.0','peft==0.8.2',\n",
      "/var/tmp/ipykernel_960137/1528039428.py:8: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  def finetuning():\n"
     ]
    }
   ],
   "source": [
    "@component(packages_to_install=['bitsandbytes==0.42.0','peft==0.8.2',\n",
    "                                'trl==0.7.10','accelerate==0.27.1',\n",
    "                                'datasets==2.17.0','transformers==4.38.0','huggingface_hub',\n",
    "                                'google-cloud-storage','google-cloud-aiplatform','google-cloud-pipeline-components',\n",
    "                                'gcsfs'],\n",
    "           base_image='gcr.io/deeplearning-platform-release/base-cu113.py310',\n",
    "           output_component_file=os.path.join(__location__, \"model_finetuning.yaml\"))\n",
    "def finetuning():\n",
    "    import torch\n",
    "    import json\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "    from datasets import load_dataset\n",
    "    from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "    import bitsandbytes as bnb\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    import transformers\n",
    "    from trl import SFTTrainer\n",
    "    from google.cloud import storage\n",
    "    from huggingface_hub import login\n",
    "    import os\n",
    "    \n",
    "    def load_json_from_gcs(bucket_name, folder_path, file_name):\n",
    "        # Initialize a client\n",
    "        client = storage.Client()\n",
    "\n",
    "        # Get the bucket\n",
    "        bucket = client.bucket(bucket_name)\n",
    "\n",
    "        # Get the blob (file) from the bucket\n",
    "        blob = bucket.blob(f\"{folder_path}/{file_name}\")\n",
    "\n",
    "        # Download the content of the blob as a string\n",
    "        json_content = blob.download_as_string()\n",
    "\n",
    "        # Decode the content to UTF-8 and load it as JSON\n",
    "        json_data = json.loads(json_content.decode('utf-8'))\n",
    "\n",
    "        return json_data\n",
    "\n",
    "    def write_json_to_local(json_data, file_name):\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(json_data, f, indent=4)\n",
    "    bucket_name = '19865_finetuned_models'\n",
    "    folder_path = 'training_data'\n",
    "    file_name = 'clean_data.json'\n",
    "    # gs://19865_finetuned_models/training_data/clean_data.json\n",
    "    # Load JSON from GCS bucket\n",
    "    json_data = load_json_from_gcs(bucket_name, folder_path, file_name)\n",
    "\n",
    "    # Write JSON data to local file with the same name\n",
    "    write_json_to_local(json_data, file_name)\n",
    "    \n",
    "    login(token='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')\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}) ## changed this line\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map='auto')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "    # dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\n",
    "    dataset = load_dataset('json', data_files=file_name)\n",
    "    \n",
    "    def upload_folder_to_gcs(local_folder_path, gcs_bucket, gcs_folder_path=\"\"):\n",
    "        # Create a GCS client and bucket object\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(gcs_bucket)\n",
    "\n",
    "        # Loop through the files in the local folder\n",
    "        for root, dirs, files in os.walk(local_folder_path):\n",
    "            for file in files:\n",
    "                # Construct the local and GCS paths for the file\n",
    "                local_path = os.path.join(root, file)\n",
    "                gcs_path = os.path.join(gcs_folder_path, local_path[len(local_folder_path)+1:])\n",
    "\n",
    "                # Upload the file to GCS\n",
    "                blob = bucket.blob(gcs_path)\n",
    "                blob.upload_from_filename(local_path)\n",
    "\n",
    "        print(f\"Folder {local_folder_path} uploaded to GCS bucket '{gcs_bucket}' with path '{gcs_folder_path}'\")\n",
    "    \n",
    "    def generate_prompt(data_point):\n",
    "        \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "        :param data_point: dict: Data point\n",
    "        :return: dict: tokenzed prompt\n",
    "        \"\"\"\n",
    "        prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n",
    "                   'appropriately completes the request.\\n\\n'\n",
    "        # Samples with additional context into.\n",
    "        if data_point['input_text']:\n",
    "            text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"input_text\"]} here are the inputs {data_point[\"Data\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output_text\"]} <end_of_turn>\"\"\"\n",
    "        # Without\n",
    "        else:\n",
    "            text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"input_text\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"Data\"]} <end_of_turn>\"\"\"\n",
    "        return text\n",
    "    def find_all_linear_names(model):\n",
    "        cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "        lora_module_names = set()\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, cls):\n",
    "                names = name.split('.')\n",
    "                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "            if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "                lora_module_names.remove('lm_head')\n",
    "        return list(lora_module_names)\n",
    "    text_column = [generate_prompt(data_point) for data_point in dataset['train']]\n",
    "    dataset = dataset['train'].add_column(\"prompt\", text_column)\n",
    "    dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "    dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    train_data = dataset[\"train\"]\n",
    "    test_data = dataset[\"test\"]\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    modules = find_all_linear_names(model)\n",
    "    lora_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    trainable, total = model.get_nb_trainable_parameters()\n",
    "    print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        dataset_text_field=\"prompt\",\n",
    "        peft_config=lora_config,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=0.03,\n",
    "            num_train_epochs=3,\n",
    "            max_steps=100,\n",
    "            learning_rate=2e-4,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            save_strategy=\"epoch\",\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "    print(\"*****training started*****\")\n",
    "    trainer.train()\n",
    "    new_model = \"gemma-Instruct-Finetune-full-acn\"\n",
    "    \n",
    "    trainer.model.save_pretrained(new_model)\n",
    "    # trainer.model.push_to_hub(new_model, use_temp_dir=False)\n",
    "    # tokenizer.push_to_hub(new_model, use_temp_dir=False)\n",
    "    # print('*****trained_model_pushed_to_hub*****')\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},)\n",
    "    \n",
    "    merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "    merged_model= merged_model.merge_and_unload()\n",
    "    \n",
    "    merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "    tokenizer.save_pretrained(\"merged_model\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "    tokenizer.push_to_hub(new_model, use_temp_dir=False)\n",
    "\n",
    "    \n",
    "    # local_path = f\"transformers/{new_model}\"\n",
    "    # trainer.model.save_pretrained(local_path)\n",
    "    \n",
    "    # upload_folder_to_gcs('transformers','19865_finetuned_models','transformers_finetuned_models')\n",
    "    # print(\"*****training_completed*****\")\n",
    "    # # trainer.model.save_pretrained(new_model)\n",
    "    \n",
    "    \n",
    "#     base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     return_dict=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     # device_map={\"\": 0},)\n",
    "#     device_map='auto',)\n",
    "    \n",
    "#     merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "#     merged_model= merged_model.merge_and_unload()\n",
    "    \n",
    "#     merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "#     tokenizer.save_pretrained(\"merged_model\")\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "#     tokenizer.padding_side = \"right\"\n",
    "    \n",
    "#     merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "#     tokenizer.push_to_hub(new_model, use_temp_dir=False)\n",
    "    \n",
    "#     local_path = f\"transformers/{new_model}\"\n",
    "#     trainer.model.save_pretrained(local_path)\n",
    "    \n",
    "#     upload_folder_to_gcs(local_path,'19865_finetuned_models','transformers_pretrained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940caecc-eff7-458d-82f6-f2128fbd3509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "from kfp import components\n",
    "transformer_component = components.load_component_from_file(\n",
    "    os.path.join(__location__,'model_finetuning.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec262c3-d730-46cc-b67f-855d51ad1fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name='gemma-finetuning',\n",
    "    description='finetuning gemma 7b',\n",
    "    # needs to be changed based on region/project\n",
    "    pipeline_root=PIPELINE_ROOT)\n",
    "def train_pipeline()->None:\n",
    "    \n",
    "    my_task = (transformer_component().set_cpu_limit('12').set_memory_limit('85G').add_node_selector_constraint('cloud-tpus.google.com/v3').set_accelerator_type('NVIDIA_TESLA_A100').set_accelerator_limit(1))\n",
    "    # my_task = (transformer_component().set_cpu_limit('4').set_memory_limit('32G').add_node_selector_constraint('cloud-tpus.google.com/v3').set_accelerator_type('NVIDIA_L4').set_accelerator_limit(1))\n",
    "    # my_task5 = \n",
    "compiler.Compiler().compile(pipeline_func=train_pipeline, package_path=\"transformer_finetuning.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088f5f9-60bc-4f4b-b679-87712d90551d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240308154439\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240308154439')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/gemma-finetuning-20240308154439?project=81995035742\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240308154439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240308154439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240308154439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240308154439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240308154439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "vertex_ai.init(project=project_id)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"transformer-finetuning-pipeline\",\n",
    "    template_path=\"transformer_finetuning.json\",location='us-central1'\n",
    ")\n",
    "job.run(service_account=service_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c11b0-0cfd-456a-9fc3-6d59fd974835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6b4f1-3534-4fa2-b954-e7d17f9d6c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m116"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
