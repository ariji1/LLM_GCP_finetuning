{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1dcfb3b-bdcd-4a10-8c54-e70b781ef37c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_files_into_string(file_list):\n",
    "    \"\"\"\n",
    "    Load text files from a list into a single string.\n",
    "\n",
    "    Args:\n",
    "        file_list (list): List of file paths.\n",
    "\n",
    "    Returns:\n",
    "        str: Contents of all files concatenated into a single string.\n",
    "    \"\"\"\n",
    "    concatenated_string = \"\"\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_contents = file.read()\n",
    "                concatenated_string += file_contents\n",
    "        except IOError:\n",
    "            print(\"Error: Unable to read file -\", file_path)\n",
    "            continue\n",
    "    return concatenated_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f55c7b-336c-46ba-bf00-343938ef91fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "file_list = [\"train_questions.txt\", \"train_questions_1.txt\", \"train_questions_2.txt\",\"train_questions_3.txt\"]\n",
    "all_text = load_files_into_string(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc5ad04d-0b59-46ec-b93e-86114a2c594c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = all_text.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7913abce-2a80-4d1a-8b84-84648cf6b049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting property name enclosed in double quotes: line 1 column 134 (char 133). Skipping the current entry.\n",
      "Error decoding JSON: Extra data: line 1 column 8 (char 7). Skipping the current entry.\n",
      "Error decoding JSON: Extra data: line 1 column 9 (char 8). Skipping the current entry.\n",
      "Error decoding JSON: Extra data: line 1 column 9 (char 8). Skipping the current entry.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def clean_and_load_to_list(string_data):\n",
    "    \"\"\"\n",
    "    Clean the string containing JSON objects and load them into a list.\n",
    "\n",
    "    Args:\n",
    "        string_data (str): String containing JSON objects.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing cleaned data.\n",
    "    \"\"\"\n",
    "    question = []\n",
    "    instruction = []\n",
    "    answer = []\n",
    "    \n",
    "    \n",
    "    cleaned_list = []\n",
    "    # Split the string by newline character to separate individual JSON objects\n",
    "    json_objects = string_data.strip().split('\\n')\n",
    "    for json_str in json_objects:\n",
    "        try:\n",
    "            # Parse each JSON object\n",
    "            json_data = json.loads(json_str)\n",
    "            # Extract required fields\n",
    "            question = json_data.get('question', 'N/A')\n",
    "            table = json_data.get('table', 'N/A')\n",
    "            answer = json_data.get('answer', 'N/A')\n",
    "            # clause = json_data.get('clause', 'N/A')\n",
    "            \n",
    "            # Append cleaned data to the list\n",
    "            # cleaned_list.append({'text':f'### Human: {question} Here is the Data : {table} ### Assistant: {answer}'})\n",
    "            \n",
    "            cleaned_list.append({\"input_text\":f'Question: {question} Here is the Data : {table}', \"output_text\": f'{answer}'})\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}. Skipping the current entry.\")\n",
    "    return cleaned_list\n",
    "\n",
    "# Example usage:\n",
    "cleaned_data = clean_and_load_to_list(all_text)\n",
    "# print(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ebae716b-d739-4713-9548-609d17d8565b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': 'Question: How are long-lived assets assessed for recoverability according to the provided financial data? Here is the Data : | Fiscal Year | Research and Development Costs | Advertising Costs | Provision for Doubtful Accounts |\\n|---|---|---|---|\\n| 2021 | $1,118,320 | $171,883 | $6,199 |\\n| 2020 | $870,611 | $57,658 | $147 |\\n| 2019 | $799,734 | $85,521 | $974 |',\n",
       " 'output_text': 'Long-lived assets or groups of assets are assessed based on a comparison of the carrying amount to the estimated future net cash flows. If estimated future undiscounted net cash flows are less than the carrying amount, the asset is considered impaired and a loss is recorded. Intangible assets with finite lives are generally amortized using the straight-line method over their estimated economic useful lives.'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed770553-3f3e-40ca-990a-5eef201f4417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '### Human: How are long-lived assets assessed for recoverability according to the provided financial data? Here is the Data : | Fiscal Year | Research and Development Costs | Advertising Costs | Provision for Doubtful Accounts |\\n|---|---|---|---|\\n| 2021 | $1,118,320 | $171,883 | $6,199 |\\n| 2020 | $870,611 | $57,658 | $147 |\\n| 2019 | $799,734 | $85,521 | $974 | ### Assistant: Long-lived assets or groups of assets are assessed based on a comparison of the carrying amount to the estimated future net cash flows. If estimated future undiscounted net cash flows are less than the carrying amount, the asset is considered impaired and a loss is recorded. Intangible assets with finite lives are generally amortized using the straight-line method over their estimated economic useful lives.'},\n",
       " {'text': '### Human: What is the question you would ask regarding the provided financial data? Here is the Data : | Three Months Ended | North America Revenue (USD) | Europe Revenue (USD) | Growth Markets Revenue (USD) |\\n|---|---|---|---|\\n| August 31, 2023 | $7,553,990 | $5,297,437 | $3,133,773 |\\n| August 31, 2022 | $7,523,505 | $4,803,237 | $3,096,914 | ### Assistant: One question that could be asked is: How did the revenues in North America, Europe, and Growth Markets compare between August 31, 2023, and August 31, 2022?'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df4639db-214f-4df3-bf7b-3f0da22b29d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"data.jsonl\"\n",
    "with open(file_path, 'w') as file:\n",
    "    for item in cleaned_data:\n",
    "        json.dump(item, file)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc11b592-3566-452e-b71d-ff9bb939e464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 95.1 KiB/ 95.1 KiB]                                                \n",
      "Operation completed over 1 objects/95.1 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp data.jsonl gs://19865_finetuned_models/training_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab81b0c7-d6f1-4ed8-a4c3-df1dcbe1ba96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9fd6ca9-4e6b-4f7c-9bda-431565f8bbb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 49, in main\n",
      "    service.run()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
      "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 113, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 189, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/opt/conda/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/opt/conda/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "  File \"/opt/conda/lib/python3.10/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(\"stevhliu/processed_demo\")dataset.push_to_hub(\"stevhliu/processed_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "012d297c-43e4-4fbb-85c0-4eeae6d0e0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('ariji1/acn-finetuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f414b06f-52cc-4918-bdaa-59311a0e2458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 154\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "345b04e5-b388-43c3-acbb-45eaf7f31018",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a858f70475da457394ca8862e7117a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/395 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edcb99754fc4c3fbae1ecd4c0688b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17472d49f34646ab993eab9e91c24e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ea2366b0df4ca298016192fb6f2aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05136d268c5c4563b66215b61f83a896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 9846\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 518\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset('timdettmers/openassistant-guanaco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8c6888f-bbf2-4b98-b37c-62caa9952f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path = gs://19865_finetuned_models/training_data/data.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8cd77d8-361f-489c-836b-710e6ab260a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '### Human: How are long-lived assets assessed for recoverability according to the provided financial data? Here is the Data : | Fiscal Year | Research and Development Costs | Advertising Costs | Provision for Doubtful Accounts |\\n|---|---|---|---|\\n| 2021 | $1,118,320 | $171,883 | $6,199 |\\n| 2020 | $870,611 | $57,658 | $147 |\\n| 2019 | $799,734 | $85,521 | $974 | ### Assistant: Long-lived assets or groups of assets are assessed based on a comparison of the carrying amount to the estimated future net cash flows. If estimated future undiscounted net cash flows are less than the carrying amount, the asset is considered impaired and a loss is recorded. Intangible assets with finite lives are generally amortized using the straight-line method over their estimated economic useful lives.'}, {'text': '### Human: What is the question you would ask regarding the provided financial data? Here is the Data : | Three Months Ended | North America Revenue (USD) | Europe Revenue (USD) | Growth Markets Revenue (USD) |\\n|---|---|---|---|\\n| August 31, 2023 | $7,553,990 | $5,297,437 | $3,133,773 |\\n| August 31, 2022 | $7,523,505 | $4,803,237 | $3,096,914 | ### Assistant: One question that could be asked is: How did the revenues in North America, Europe, and Growth Markets compare between August 31, 2023, and August 31, 2022?'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Read a JSONL file and return a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"data.jsonl\"\n",
    "jsonl_data = read_jsonl(file_path)\n",
    "print(jsonl_data[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56777523-9442-4ee3-8346-f73c46cf4499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ariji1/acn-finetuning\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa8cea26-9e8a-45a6-b5a8-9910593766cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 154\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m116"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
