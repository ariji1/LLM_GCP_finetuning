{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqlwNikNPFRW"
   },
   "source": [
    "# Fine Tuning GPT-3.5-Turbo\n",
    "\n",
    "In this notebook, we walk through an example of fine-tuning gpt-3.5-turbo.\n",
    "\n",
    "Specifically, we attempt to distill GPT-4's knowledge, by generating training data with GPT-4 to then fine-tune GPT-3.5.\n",
    "\n",
    "All training data is generated using two different sections of our index data, creating both a training and evalution set.\n",
    "\n",
    "We then finetune with our `OpenAIFinetuneEngine` wrapper abstraction.\n",
    "\n",
    "Evaluation is done using the `ragas` library, which we will detail later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google\n",
    "creds,project = google.auth.default()\n",
    "project_id = \"sbx-196865-genaift-ds-ccd784e6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creds.project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = Vertex(model=\"gemini-pro\", project=project_id, credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# help(Vertex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wfpi7nDbPFRY",
    "outputId": "24c9baef-36ca-4c23-f2de-ff5e88d43226",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-finetuning\n",
    "%pip install llama-index-finetuning-callbacks\n",
    "%pip install llama-index-llms-openai\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0yTRCEMSbpi",
    "outputId": "8ad455fc-1577-4c81-90e1-4e3bc9dc3b30",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTmX3A_UPFRa",
    "outputId": "0999317f-fb48-4a60-bdc1-5157ae9ffa90",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install  pypdf sentence-transformers ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPY8tyVNQm0u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcBJ9wBRQnfH",
    "outputId": "3e1b1d42-88d9-45ef-c819-1c714d2b3f60",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35KDf-WcPFRa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5NpYdM1PFRa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkjjtEL9RPlY",
    "outputId": "a4b6f2c2-20d1-4709-c119-fe8b6b95dfa2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EF1duTucRYzB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eCciMMfPFRa"
   },
   "source": [
    "## Data Setup\n",
    "\n",
    "Here, we first down load the PDF that we will use to generate training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWwHunT2PFRa",
    "outputId": "7b63271b-1ac2-46a6-fba4-d11ac2a4d4af",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d7EgQ17PFRb"
   },
   "source": [
    "The next step is generating a training and eval dataset.\n",
    "\n",
    "We will generate 40 questions on different sections of the PDF we downloaded.\n",
    "\n",
    "We can use GPT-3.5 on the eval questions to get our baseline performance.\n",
    "\n",
    "Then, we will use GPT-4 on the train questions to generate our training data. The training data will be collected with out `OpenAIFineTuningHandler`.\n",
    "\n",
    "This step is entirely optional if you don't want to spend the time/tokens -- the eval and training questions are also provided in this folder, as well as the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LK4mFVoGPFRb"
   },
   "source": [
    "### Train Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "rN8qD-zgPFRb",
    "outputId": "8c848f77-4624-4be8-b1c7-8bd924ca1fb4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"accenture-reports-first-quarter-fiscal-2024-results.pdf\",\n",
    "                 \"accentures-fourth-quarter-fiscal-2022-earnings-release.pdf\",\n",
    "                 \"final-q4-fy23-earnings-press-release.pdf\",\n",
    "                \"A061213R.pdf\",\n",
    "                \"accenture-fiscal-2021-annual-report.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "# Shuffle the documents\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(documents)\n",
    "\n",
    "gpt_35_llm = OpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_35_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6G2M6gWQPFRb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_gen_query = (\n",
    "    \"You are a Financial expert in Accenture C suite. Your task is to setup \"\n",
    "    \"a set of questions and capture the context in tabular format for the given question as per the format of examples given below from the \"\n",
    "    \"Accenture finance reports, formulate \"\n",
    "    \"a single question based on the tabular data that you extract from the\\\n",
    "    context. Restrict the question to the context information that you are able to extract. Please include numbers in your response. Make sure that the data to the columns present in the tabular data format, include only numbers\"\n",
    "    \"Put numbers only in the data as you are able to extract from the context, do not interpolate or calculate any values. Extract only the accurate values from the data.\"\n",
    "    \"Return the extracted question and context as a single string enclosed in a list as [{question:<put the question here>,context:<put the context here>}]\"\n",
    "    \"here are few examples:\"\n",
    "    \"[{question: Generate a market financial narrative using the context,\\\n",
    "    context: Market | Revenue | Revenue to Plan | Sales | Sales to Plan | Profit | Profit to Plan | Costs | Costs to Plan | Backlog | Backlog to Plan\\\n",
    "North America | $5.3M | -12% | $3.4M | 9% | $4.9M | 5% | $2.1M | 15% | $7.2M | 30%\\\n",
    "Europe | $6.7M | 25% | $7.1M | -20% | $8.3M | -35% | $4.8M | -25% | $5.9M | -10%\\\n",
    "Asia | $8.2M | -30% | $9.8M | 15% | $7.6M | 20% | $6.3M | 10% | $4.5M | -18%},\"\n",
    "\"{question:Write a financial report of Technology services based on the context,\\\n",
    "context: Technology Services\\\n",
    "Service Group | Bookings ($) | Bookings YoY (%) | Revenue ($) | Revenue YoY (%) | Profit ($) | Profit YoY (%) | Sales ($) | Sales YoY (%)\\\n",
    "SI | $14.3M | -1.2% | $6.2M | -4.1% | $3.6M | -2.9% | $6.0M | 0.3%\\\n",
    "AMS | $13.7M | 2.8% | $4.8M | 9.5% | $4.5M | 4.9% | $3.6M | -14.4%\\\n",
    "IMS | $8.1M | 5.9% | $6.6M | -2.9% | $4.1M | -5.4% | $2.0M | -7.1%}]\")\n",
    "\n",
    "preamble = \"### Finance Report Summary ### Human: You are a financial analyst reviewing a report. Your task is to formulate a question based on the data provided.\\\n",
    "Provide detailed and accurate questions and answers. ### Assistant: Assume the role of an expert analyst. Generate the tabular data required to answer the question here. \\\n",
    "Provide only numbers inside the columns. ### Assistant: Assume you are a financial expert. Create the answer or summary based on the question here. Provide detailed and crisp answers.\\\", \"\n",
    "\n",
    "question_gen_query = (\n",
    "    \"\"\"Assume you are a financial analyst tasked with extracting insights from a finance report. \"\"\"\n",
    "    \"\"\"Your goal is to generate questions and answers based on the provided text. If the question is not related to finance, respond with 'This is not related to finance'.The format should adhere to the following structure:\"\"\"\n",
    "    # \"\"\"{\\\"text:\"\"\"\n",
    "    \"\"\"{\\\"question\\\": \\\"What is the question you would ask regarding the provided financial data?\\\", \"\"\"\n",
    "    \"\"\"\\\"table\\\": \\\"| Sample Column 1 | Sample Column 2 | Sample Column 3 |\\\\n|---|---|---|\\\\n| Sample Data 1 | Sample Data 2 | Sample Data 3 |\\\", \"\"\"\n",
    "    \"\"\"\\\"answer\\\": \\\"Your detailed and crisp answer or summary goes here.\\\", \"\"\"\n",
    "    \"\"\"\\\"clause\\\": \\\"If the question is not related to finance, respond with 'This is not related to finance'.\\\"}\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents[150:],\n",
    "    question_gen_query=question_gen_query,\n",
    "    # llm=llm\n",
    "    llm=gpt_35_llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUYtfHm_hBId",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message =question_gen_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoRq6zDbPFRb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: this may take some time. Go grab a coffee!\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=50)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-J91UroPFRc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"train_questions_3.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9JNHFNFRy6Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HofLhpwFPFRc"
   },
   "source": [
    "### Eval Generation\n",
    "\n",
    "Now, lets generate questions on a completely different set of documents, in order to create our eval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_3I2xwpPFRc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents[\n",
    "        23:\n",
    "    ],  # since we generated ~1 question for 40 documents, we can skip the first 40\n",
    "    question_gen_query=question_gen_query,\n",
    "    llm=gpt_35_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbqjgmmSPFRc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: this may take some time. Go grab a coffee!\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lzhzj4IvPFRc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"eval_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CucofX-OSBBM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H6EY8JhPFRc"
   },
   "source": [
    "## Initial Eval with GPT-3.5-Turbo Query Engine\n",
    "\n",
    "For this eval, we will be using the [`ragas` evaluation library](https://github.com/explodinggradients/ragas).\n",
    "\n",
    "Ragas has a ton of evaluation metrics for RAG pipelines, and you can read about them [here](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md).\n",
    "\n",
    "For this notebook, we will be using the following two metrics\n",
    "\n",
    "- `answer_relevancy` - This measures how relevant is the generated answer to the prompt. If the generated answer is incomplete or contains redundant information the score will be low. This is quantified by working out the chance of an LLM generating the given question using the generated answer. Values range (0,1), higher the better.\n",
    "- `faithfulness` - This measures the factual consistency of the generated answer against the given context. This is done using a multi step paradigm that includes creation of statements from the generated answer followed by verifying each of these statements against the context. The answer is scaled to (0,1) range. Higher the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MvzZtTdPFRd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Oh2rm7ePFRd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# limit the context window to 2048 tokens so that refine is used\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.context_window = 2048\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2, llm=gpt_35_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6umV0qiZPFRd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTSy68LqPFRd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuoPUQoBEhUH",
    "outputId": "7ebbe47d-630e-44f3-d5ee-f01031e8153e"
   },
   "outputs": [],
   "source": [
    "#how to run a custom model\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:personal:acnearningsreport:8xaOhqJD\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a geography geek\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of burundy\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feFQGUhNPFRd"
   },
   "source": [
    "## GPT-4 to Collect Training Data\n",
    "\n",
    "Here, we use GPT-4 and the `OpenAIFineTuningHandler` to collect data that we want to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZOS5tcePFRd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.finetuning.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "llm.callback_manager = callback_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROBiodIxaE4C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4L_57EeaEr_",
    "outputId": "16fceade-c4ba-4562-afea-b4987fb383bd"
   },
   "outputs": [],
   "source": [
    "finetuning_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0E2t3Rc5aEOy"
   },
   "outputs": [],
   "source": [
    "finetuning_handler.get_finetuning_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGBlgUiGPFRd"
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"train_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXytwdt3VhIu"
   },
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqBlUyT0PFRd"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNnnhCTQgmui"
   },
   "outputs": [],
   "source": [
    "def generate_chat_output(system_message, user_messages, response_messages):\n",
    "    # Validate input lengths\n",
    "    if len(user_messages) != len(response_messages):\n",
    "        raise ValueError(\"The length of user_messages and response_messages must be equal.\")\n",
    "\n",
    "    output = []  # List to hold the output dictionaries\n",
    "\n",
    "    for user_message, response_message in zip(user_messages, response_messages):\n",
    "        # Construct the message group\n",
    "        message_group = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": response_message}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Append the constructed group to the output list\n",
    "        output.append(message_group)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Test the function\n",
    "system_message = \"Marv is a factual chatbot that is also sarcastic.\"\n",
    "user_messages = [\"What's the capital of France?\", \"Who wrote 'Romeo and Juliet'?\", \"How far is the Moon from Earth?\"]\n",
    "response_messages = [\n",
    "    \"Paris, as if everyone doesn't know that already.\",\n",
    "    \"Oh, just some guy named William Shakespeare. Ever heard of him?\",\n",
    "    \"Around 384,400 kilometers. Give or take a few, like that really matters.\"\n",
    "]\n",
    "\n",
    "# Generate and print the output\n",
    "output = generate_chat_output(system_message, user_messages, response_messages)\n",
    "for message_group in output:\n",
    "    print(message_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJVLqKSAPFRd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "user_messages=[]\n",
    "response_messages=[]\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    print (question)\n",
    "    user_messages.append(question)\n",
    "    response_messages.append(str(response))\n",
    "    print(response)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRgydKUWk9b4"
   },
   "outputs": [],
   "source": [
    "evalquestions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        evalquestions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyilfzaSlB0H"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "user_messageseval=[]\n",
    "response_messageseval=[]\n",
    "for question in evalquestions:\n",
    "    response = query_engine.query(question)\n",
    "    print (question)\n",
    "    user_messageseval.append(question)\n",
    "    response_messageseval.append(str(response))\n",
    "    print(response)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "Z4k55-a2hjLn",
    "outputId": "ee6ea091-3d6d-4533-cd42-b39088fe1d7e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "output = generate_chat_output(system_message, user_messages, response_messages)\n",
    "\n",
    "# Convert the output to a JSON string\n",
    "json_output = json.dumps(output, indent=4)\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"finetuning_events.json\", \"w\") as json_file:\n",
    "    json_file.write(json_output)\n",
    "\n",
    "# Save to a JSONL file\n",
    "with open(\"finetuning_events.jsonl\", \"w\") as jsonl_file:\n",
    "    for item in output:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Output saved in both JSON and JSONL formats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5to9rcYCmAku"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "output = generate_chat_output(system_message, user_messageseval, response_messageseval)\n",
    "\n",
    "# Convert the output to a JSON string\n",
    "json_output = json.dumps(output, indent=4)\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"eval_events.json\", \"w\") as json_file:\n",
    "    json_file.write(json_output)\n",
    "\n",
    "# Save to a JSONL file\n",
    "with open(\"eval_events.jsonl\", \"w\") as jsonl_file:\n",
    "    for item in output:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Output saved in both JSON and JSONL formats.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OilVbzpJPFRd"
   },
   "source": [
    "## Create `OpenAIFinetuneEngine`\n",
    "\n",
    "We create an `OpenAIFinetuneEngine`: the finetune engine will take care of launching a finetuning job, and returning an LLM model that you can directly plugin to the rest of LlamaIndex workflows.\n",
    "\n",
    "We use the default constructor, but we can also directly pass in our finetuning_handler into this engine with the `from_finetuning_handler` class method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHC2C69ePFRd"
   },
   "outputs": [],
   "source": [
    "finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--jV9ihlPFRd"
   },
   "outputs": [],
   "source": [
    "from llama_index.finetuning import OpenAIFinetuneEngine\n",
    "\n",
    "finetune_engine = OpenAIFinetuneEngine(\n",
    "    \"gpt-3.5-turbo-0125\",\n",
    "    \"finetuning_events.jsonl\",\n",
    "    # start_job_id=\"<start-job-id>\"  # if you have an existing job, can specify id here\n",
    ")\n",
    "\n",
    "# finetune_engine = OpenAIFinetuneEngine.from_finetuning_handler(\n",
    "#     finetuning_handler,\n",
    "#     \"gpt-3.5-turbo\",\n",
    "#     \"tmp.jsonl\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnsYKOsncZjt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0jFWwtWPFRd",
    "outputId": "2b1296b4-e5d5-4f45-942d-89625cccd413"
   },
   "outputs": [],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOY7icvTPFRd",
    "outputId": "5651f90e-eed0-4f92-be31-6174659d9623"
   },
   "outputs": [],
   "source": [
    "finetune_engine.get_current_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "3xm_O3yRPFRe",
    "outputId": "18a4aa3f-4166-4713-f567-ebe1f97443ac"
   },
   "outputs": [],
   "source": [
    "ft_llm = finetune_engine.get_finetuned_model(temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up-al5-iPFRe"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "After some time, your model will be done training!\n",
    "\n",
    "The next step is running our fine-tuned model on our eval dataset again to measure any performance increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtDslF8sPFRe"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.finetuning.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "\n",
    "\n",
    "# Option 1: pass in ft_llm directly into Settings\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = ft_llm\n",
    "Settings.context_window = (\n",
    "    2048  # limit the context window artifically to test refine process\n",
    ")\n",
    "\n",
    "# # Option 2: you can also specify the model name manually\n",
    "# ft_model_name = \"ft:gpt-3.5-turbo-0613:...\"\n",
    "# Settings.llm = OpenAI(model=ft_model_name, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfSeSjJbPFRe"
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDbzXDYFPFRe"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2, llm=ft_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItoKViezPFRe"
   },
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVRy2Qali8sf"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ooy_ZBZxPFRe"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtvCTyg7PFRi"
   },
   "source": [
    "## Exploring Differences\n",
    "\n",
    "Let's quickly compare the differences in responses, to demonstrate that fine tuning did indeed change something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4asVoAwkPFRi"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGybEbhNPFRi"
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ds0SaTCPFRi"
   },
   "outputs": [],
   "source": [
    "print(questions[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSE-NHf0PFRj"
   },
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xVdRV-0PFRj"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "gpt_35_llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wwe9M0SuPFRj"
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=gpt_35_llm)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAjC3hpUPFRj"
   },
   "source": [
    "### Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbUgU4kHPFRj"
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=ft_llm)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-XNraP0PFRj"
   },
   "source": [
    "As we can see, the fine-tuned model provides a more thorough response! This lines up with the increased faithfullness score from ragas, since the answer is more representative of the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u8TVkk3PFRj"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "So, in conclusion, finetuning with only ~61 questions actually helped improve our eval scores!\n",
    "\n",
    "**answer_relevancy: 0.9725 -> 0.9607**\n",
    "\n",
    "The answer relevancy dips slightly but it's very small.\n",
    "\n",
    "**faithfulness: 0.7325 -> 0.7917**\n",
    "\n",
    "The faithfulness appears to have been improved! This mains the anwers given better fuffil the original question that was asked."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m116"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
