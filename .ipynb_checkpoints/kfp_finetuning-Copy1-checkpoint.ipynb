{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecc91a00-29d2-46cd-a067-bc56d82db6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import List, Tuple\n",
    "\n",
    "import os\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component,pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5a79045-87d0-4306-a96e-5ecf6a0300f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI=\"gs://sbx-196865-genaift-ds-pkgs\"\n",
    "project_id = \"sbx-196865-genaift-ds-ccd784e6\"\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/\".format(BUCKET_URI)\n",
    "EXPERIMENT_NAME = \"test-1\"\n",
    "location = 'us-central1'\n",
    "service_account=\"sa-196865-big-data@sbx-196865-genaift-ds-ccd784e6.iam.gserviceaccount.com\"\n",
    "model_display_name = 'tuned_bison001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69a4d8ab-65e5-42d3-ab15-e639161af04c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "__file__ = 'kfp_finetuning.ipynb'\n",
    "__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "599bbe63-c94b-4760-a82d-4834c5f667dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import List, Tuple\n",
    "\n",
    "import os\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component,pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "045a4b26-2ea8-419b-9a78-69df1db46ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_134885/3531137494.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(packages_to_install=['bitsandbytes==0.42.0','peft==0.8.2',\n",
      "/var/tmp/ipykernel_134885/3531137494.py:8: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  def finetuning():\n"
     ]
    }
   ],
   "source": [
    "@component(packages_to_install=['bitsandbytes==0.42.0','peft==0.8.2',\n",
    "                                'trl==0.7.10','accelerate==0.27.1',\n",
    "                                'datasets==2.17.0','transformers==4.38.0','huggingface_hub',\n",
    "                                'google-cloud-storage','google-cloud-aiplatform','google-cloud-pipeline-components',\n",
    "                                'gcsfs'],\n",
    "           base_image='gcr.io/deeplearning-platform-release/base-cu113.py310',\n",
    "           output_component_file=os.path.join(__location__, \"model_finetuning.yaml\"))\n",
    "def finetuning():\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "    from datasets import load_dataset\n",
    "    from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "    import bitsandbytes as bnb\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    import transformers\n",
    "    from trl import SFTTrainer\n",
    "    from google.cloud import storage\n",
    "    from huggingface_hub import login\n",
    "    import os\n",
    "    \n",
    "    login(token='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')\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    ) \n",
    "    \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map='auto') ## changed this line\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "    dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\n",
    "    \n",
    "    def upload_folder_to_gcs(local_folder_path, gcs_bucket, gcs_folder_path=\"\"):\n",
    "        # Create a GCS client and bucket object\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(gcs_bucket)\n",
    "\n",
    "        # Loop through the files in the local folder\n",
    "        for root, dirs, files in os.walk(local_folder_path):\n",
    "            for file in files:\n",
    "                # Construct the local and GCS paths for the file\n",
    "                local_path = os.path.join(root, file)\n",
    "                gcs_path = os.path.join(gcs_folder_path, local_path[len(local_folder_path)+1:])\n",
    "\n",
    "                # Upload the file to GCS\n",
    "                blob = bucket.blob(gcs_path)\n",
    "                blob.upload_from_filename(local_path)\n",
    "\n",
    "        print(f\"Folder {local_folder_path} uploaded to GCS bucket '{gcs_bucket}' with path '{gcs_folder_path}'\")\n",
    "    \n",
    "    def generate_prompt(data_point):\n",
    "        \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "        :param data_point: dict: Data point\n",
    "        :return: dict: tokenzed prompt\n",
    "        \"\"\"\n",
    "        prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n",
    "                   'appropriately completes the request.\\n\\n'\n",
    "        # Samples with additional context into.\n",
    "        if data_point['input']:\n",
    "            text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n",
    "        # Without\n",
    "        else:\n",
    "            text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n",
    "        return text\n",
    "    def find_all_linear_names(model):\n",
    "        cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "        lora_module_names = set()\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, cls):\n",
    "                names = name.split('.')\n",
    "                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "            if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "                lora_module_names.remove('lm_head')\n",
    "        return list(lora_module_names)\n",
    "    text_column = [generate_prompt(data_point) for data_point in dataset]\n",
    "    dataset = dataset.add_column(\"prompt\", text_column)\n",
    "    dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "    dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    train_data = dataset[\"train\"]\n",
    "    test_data = dataset[\"test\"]\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    modules = find_all_linear_names(model)\n",
    "    lora_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    trainable, total = model.get_nb_trainable_parameters()\n",
    "    print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        dataset_text_field=\"prompt\",\n",
    "        peft_config=lora_config,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=0.03,\n",
    "            max_steps=100,\n",
    "            learning_rate=2e-4,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            save_strategy=\"epoch\",\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "    trainer.train()\n",
    "    new_model = \"gemma-Code-Instruct-Finetune-test-acn\"\n",
    "    trainer.model.save_pretrained(new_model)\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},)\n",
    "    \n",
    "    merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "    merged_model= merged_model.merge_and_unload()\n",
    "    \n",
    "    merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "    tokenizer.save_pretrained(\"merged_model\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "    tokenizer.push_to_hub(new_model, use_temp_dir=False)\n",
    "    \n",
    "    local_path = f\"transformers/{new_model}\"\n",
    "    trainer.model.save_pretrained(local_path)\n",
    "    \n",
    "    upload_folder_to_gcs(local_path,'19865_finetuned_models','transformers_pretrained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "940caecc-eff7-458d-82f6-f2128fbd3509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "from kfp import components\n",
    "transformer_component = components.load_component_from_file(\n",
    "    os.path.join(__location__,'model_finetuning.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ec262c3-d730-46cc-b67f-855d51ad1fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name='gemma-finetuning',\n",
    "    description='finetuning gemma 7b',\n",
    "    # needs to be changed based on region/project\n",
    "    pipeline_root=PIPELINE_ROOT)\n",
    "def train_pipeline()->None:\n",
    "    my_task = (transformer_component().set_cpu_limit('12').set_memory_limit('16G').add_node_selector_constraint('cloud-tpus.google.com/v3').set_accelerator_type('NVIDIA_TESLA_T4').set_accelerator_limit(2))\n",
    "    # my_task5 = \n",
    "compiler.Compiler().compile(pipeline_func=train_pipeline, package_path=\"transformer_finetuning.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088f5f9-60bc-4f4b-b679-87712d90551d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/gemma-finetuning-20240306090709?project=81995035742\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/81995035742/locations/us-central1/pipelineJobs/gemma-finetuning-20240306090709 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "vertex_ai.init(project=project_id)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"transformer-finetuning-pipeline\",\n",
    "    template_path=\"transformer_finetuning.json\",location='us-central1'\n",
    ")\n",
    "job.run(service_account=service_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c11b0-0cfd-456a-9fc3-6d59fd974835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6b4f1-3534-4fa2-b954-e7d17f9d6c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m116"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
